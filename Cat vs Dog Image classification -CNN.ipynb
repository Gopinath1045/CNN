{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "68c0d655",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imporing necessary libraries\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten,Conv2D, MaxPooling2D,BatchNormalization\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator , array_to_img, img_to_array, load_img\n",
    "from tensorflow.keras.preprocessing import image\n",
    "img_width, img_height = 150, 150\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0cd28b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Preprocessing\n",
    "\n",
    "#train_generator = load_img(\"F:/gopimg/gopi1img.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bd5039ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22507 images belonging to 2 classes.\n",
      "Found 2140 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "\n",
    "                                   shear_range = 0.2,\n",
    "\n",
    "                                   zoom_range = 0.2,\n",
    "\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\"F:\\gopimg\\CD_train\",\n",
    "\n",
    "                                                 target_size = (64,64),\n",
    "\n",
    "                                                 batch_size = 32,\n",
    "\n",
    "                                                 class_mode = 'binary')\n",
    "\n",
    "test_datagen=ImageDataGenerator(rescale=1./255,\n",
    "                                \n",
    "                                shear_range = 0.2,\n",
    "\n",
    "                                zoom_range = 0.2,\n",
    "\n",
    "                                horizontal_flip = True)\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\"F:\\gopimg\\Cado_test\",\n",
    "\n",
    "                                            target_size = (64,64),\n",
    "\n",
    "                                            batch_size = 32,\n",
    "\n",
    "                                            class_mode = 'binary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c04f3d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build the CNN Model\n",
    "\n",
    "#initialize the model\n",
    "\n",
    "cnn=tf.keras.models.Sequential()\n",
    "\n",
    "#Convolution\n",
    "\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32,kernel_size=3,activation='relu', input_shape=[64,64,3]))\n",
    "\n",
    "#Pooling\n",
    "\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))\n",
    "\n",
    "#Adding one more Convolution layer\n",
    "\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32,kernel_size=3,activation='relu'))\n",
    "\n",
    "# Adding one more Pooling Layer\n",
    "\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))\n",
    "\n",
    "#Flatening\n",
    "\n",
    "cnn.add(tf.keras.layers.Flatten())\n",
    "\n",
    "#Full Connection Layer\n",
    "\n",
    "cnn.add(tf.keras.layers.Dense(units=128,activation='relu'))\n",
    "\n",
    "#Full Connection Layer\n",
    "\n",
    "cnn.add(tf.keras.layers.Dense(units=128,activation='relu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "386086a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_22 (Conv2D)          (None, 62, 62, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_22 (MaxPoolin  (None, 31, 31, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_23 (Conv2D)          (None, 29, 29, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_23 (MaxPoolin  (None, 14, 14, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_11 (Flatten)        (None, 6272)              0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 128)               802944    \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 829,600\n",
      "Trainable params: 829,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#compile the model\n",
    "\n",
    "cnn.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f47901",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The above output shows that the number of trainable parameters is 813,217,\n",
    "#which can be reduced by adding more convolutional and pooling layers. With the increase in the number of layers,\n",
    "#the features extracted will be more specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5e7c4715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "704/704 [==============================] - 293s 415ms/step - loss: 2.0247 - accuracy: 0.0062 - val_loss: 0.5053 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x86bf127880>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the model\n",
    "cnn.fit(x= train_generator , validation_data=validation_generator,epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8af61c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it as a h5 file\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "cnn.save('F:\\gopimg\\model_rcat_dog.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "962f4206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_22 (Conv2D)          (None, 62, 62, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_22 (MaxPoolin  (None, 31, 31, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_23 (Conv2D)          (None, 29, 29, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_23 (MaxPoolin  (None, 14, 14, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_11 (Flatten)        (None, 6272)              0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 128)               802944    \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 829,600\n",
      "Trainable params: 829,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7f785856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 125ms/step\n"
     ]
    }
   ],
   "source": [
    "# Part 4 - Making a single prediction\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "test_image = image.load_img('F:/gopimg/gopiimg.jpg', target_size = (64,64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image=test_image/255\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = cnn.predict(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d39df45f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.6901961 , 0.8       , 0.88235295],\n",
       "         [0.6901961 , 0.8       , 0.88235295],\n",
       "         [0.6901961 , 0.8       , 0.88235295],\n",
       "         ...,\n",
       "         [0.65882355, 0.79607844, 0.8745098 ],\n",
       "         [0.65882355, 0.79607844, 0.8745098 ],\n",
       "         [0.65882355, 0.79607844, 0.8745098 ]],\n",
       "\n",
       "        [[0.69803923, 0.80784315, 0.8901961 ],\n",
       "         [0.7019608 , 0.8117647 , 0.89411765],\n",
       "         [0.70980394, 0.8117647 , 0.8784314 ],\n",
       "         ...,\n",
       "         [0.6627451 , 0.8       , 0.8784314 ],\n",
       "         [0.65882355, 0.79607844, 0.8745098 ],\n",
       "         [0.65882355, 0.79607844, 0.8745098 ]],\n",
       "\n",
       "        [[0.7019608 , 0.80784315, 0.88235295],\n",
       "         [0.7058824 , 0.8117647 , 0.8862745 ],\n",
       "         [0.7176471 , 0.81960785, 0.8862745 ],\n",
       "         ...,\n",
       "         [0.67058825, 0.79607844, 0.8784314 ],\n",
       "         [0.67058825, 0.80784315, 0.8862745 ],\n",
       "         [0.6627451 , 0.8       , 0.8784314 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.00392157, 0.02745098, 0.02745098],\n",
       "         [0.01960784, 0.03137255, 0.05098039],\n",
       "         [0.02352941, 0.05490196, 0.06666667],\n",
       "         ...,\n",
       "         [0.22745098, 0.2627451 , 0.2509804 ],\n",
       "         [0.4117647 , 0.42745098, 0.42352942],\n",
       "         [0.5882353 , 0.6039216 , 0.6       ]],\n",
       "\n",
       "        [[0.00392157, 0.03529412, 0.04705882],\n",
       "         [0.04705882, 0.09019608, 0.10588235],\n",
       "         [0.        , 0.02352941, 0.04313726],\n",
       "         ...,\n",
       "         [0.13725491, 0.18039216, 0.15686275],\n",
       "         [0.42745098, 0.43137255, 0.4117647 ],\n",
       "         [0.4862745 , 0.5019608 , 0.5058824 ]],\n",
       "\n",
       "        [[0.05098039, 0.09411765, 0.10980392],\n",
       "         [0.01960784, 0.02745098, 0.01568628],\n",
       "         [0.00784314, 0.02352941, 0.02745098],\n",
       "         ...,\n",
       "         [0.05882353, 0.09411765, 0.11372549],\n",
       "         [0.16862746, 0.20392157, 0.23921569],\n",
       "         [0.2901961 , 0.34509805, 0.39607844]]]], dtype=float32)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117d371d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ea7ed875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.24241635, 0.26895687, 0.31124535, 0.28128076, 0.30492648,\n",
       "        0.        , 0.        , 0.2827862 , 0.31999224, 0.3158979 ,\n",
       "        0.40765703, 0.27829385, 0.3045789 , 0.33188862, 0.22518289,\n",
       "        0.31147182, 0.        , 0.        , 0.35332382, 0.35114044,\n",
       "        0.29180503, 0.32526317, 0.37423667, 0.35309312, 0.2778579 ,\n",
       "        0.31073436, 0.38436723, 0.32280102, 0.33273378, 0.        ,\n",
       "        0.31740403, 0.2757157 , 0.316376  , 0.        , 0.32587057,\n",
       "        0.34926835, 0.36912766, 0.29124305, 0.        , 0.27464068,\n",
       "        0.29644015, 0.36927947, 0.29334545, 0.36398703, 0.        ,\n",
       "        0.2655272 , 0.32289237, 0.263965  , 0.37212124, 0.30708775,\n",
       "        0.2953439 , 0.31125262, 0.3956938 , 0.30837172, 0.23144576,\n",
       "        0.33327147, 0.37018594, 0.31506065, 0.3079138 , 0.318836  ,\n",
       "        0.3039004 , 0.        , 0.2527143 , 0.34967482, 0.3011521 ,\n",
       "        0.24079922, 0.        , 0.37279347, 0.34576252, 0.30254346,\n",
       "        0.319442  , 0.29714033, 0.3335905 , 0.33798358, 0.32040414,\n",
       "        0.3348683 , 0.34880337, 0.27285177, 0.32461435, 0.32089853,\n",
       "        0.26663715, 0.        , 0.30564904, 0.25516602, 0.28988278,\n",
       "        0.32823876, 0.35325906, 0.35154936, 0.1907985 , 0.29261622,\n",
       "        0.28436077, 0.31260875, 0.30099285, 0.28588182, 0.20576236,\n",
       "        0.3030843 , 0.36378258, 0.38318536, 0.34988856, 0.        ,\n",
       "        0.32676923, 0.26726472, 0.37602612, 0.        , 0.31912866,\n",
       "        0.23901068, 0.29198074, 0.35604596, 0.2994573 , 0.3179725 ,\n",
       "        0.29206032, 0.35450244, 0.30491722, 0.34459132, 0.3250338 ,\n",
       "        0.        , 0.26427487, 0.        , 0.35059014, 0.        ,\n",
       "        0.28666198, 0.34121576, 0.34180108, 0.28326026, 0.        ,\n",
       "        0.33004043, 0.        , 0.06041468]], dtype=float32)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0bcf721c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\GOPINA~1\\AppData\\Local\\Temp/ipykernel_616/1086115153.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m<=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The image classified is cat\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The image classified is dog\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "if result[0]<=0.5:\n",
    "    print(\"The image classified is cat\")\n",
    "else:\n",
    "    print(\"The image classified is dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c57982f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
